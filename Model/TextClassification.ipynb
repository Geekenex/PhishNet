{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0dbebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import DistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "420a492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9537ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetfolder = dataset + \"\\Datasets\\SpamDataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6daaa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasetfolder + \"\\spam.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7376890c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Projects\\\\ScamDetector\\\\Model\\\\Datasets\\\\SpamDataset\\\\spam.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34aa319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ff4d691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2 Unnamed: 2  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "...    ...                                                ...        ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
       "5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
       "5571   ham                         Rofl. Its true to its name        NaN   \n",
       "\n",
       "     Unnamed: 3 Unnamed: 4  \n",
       "0           NaN        NaN  \n",
       "1           NaN        NaN  \n",
       "2           NaN        NaN  \n",
       "3           NaN        NaN  \n",
       "4           NaN        NaN  \n",
       "...         ...        ...  \n",
       "5567        NaN        NaN  \n",
       "5568        NaN        NaN  \n",
       "5569        NaN        NaN  \n",
       "5570        NaN        NaN  \n",
       "5571        NaN        NaN  \n",
       "\n",
       "[5572 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fb9b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.columns[[2,3,4]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22376037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['label', 'sms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eecc480d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                                sms\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham              Will Ì_ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4142b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spamtoham(x):\n",
    "    if (x == 'spam'):\n",
    "        return 1\n",
    "    if (x== 'ham'):\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af55ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].apply(spamtoham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ddaff58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                                sms\n",
       "0         0  Go until jurong point, crazy.. Available only ...\n",
       "1         0                      Ok lar... Joking wif u oni...\n",
       "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         0  U dun say so early hor... U c already then say...\n",
       "4         0  Nah I don't think he goes to usf, he lives aro...\n",
       "...     ...                                                ...\n",
       "5567      1  This is the 2nd time we have tried 2 contact u...\n",
       "5568      0              Will Ì_ b going to esplanade fr home?\n",
       "5569      0  Pity, * was in mood for that. So...any other s...\n",
       "5570      0  The guy did some bitching but I acted like i'd...\n",
       "5571      0                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a2a94a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.134063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.340751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label\n",
       "count  5572.000000\n",
       "mean      0.134063\n",
       "std       0.340751\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.000000\n",
       "75%       0.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f84f4515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">sms</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sms                                                               \n",
       "      count unique                                                top freq\n",
       "label                                                                     \n",
       "0      4825   4516                             Sorry, I'll call later   30\n",
       "1       747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "023b7435",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15148\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='label', ylabel='count'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.countplot(df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6495f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=11, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbe140ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sms = train['sms'].values\n",
    "train_labels = train['label'].values\n",
    "\n",
    "test_sms = test['sms'].values\n",
    "test_labels = test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "829190d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "990c8724",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tokenizer(list(train_sms), truncation = True, padding = True)\n",
    "test_tokenized = tokenizer(list(test_sms), truncation = True, padding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "604a1833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=238, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b0da5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScamDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['label'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ScamDataset(train_tokenized, train_labels)\n",
    "test_dataset = ScamDataset(test_tokenized, test_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef385d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 16, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 16, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0637ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e20c60f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72889c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "529e40a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "387df9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2909a4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5531bcdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2+cu121'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485e9751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479deb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "859e1c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x173a6765a30>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "745cc790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a55ab88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute accuracy\n",
    "def accuracy(model, data_loader, device):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        correct=0\n",
    "        total=0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_masks = batch['attention_mask'].to(DEVICE)\n",
    "            label = batch['label'].to(DEVICE)\n",
    "            \n",
    "            outputs = model(input_ids, attention_masks, labels = label)\n",
    "            loss, logits = outputs['loss'], outputs['logits']\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "                \n",
    "            \n",
    "            total += label.size(0)\n",
    "            correct += (predicted_labels == label).sum()\n",
    "    \n",
    "    return correct/total *100\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3a24df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd2c9786",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001/0001 | Batch 0000/0279 | Loss: 0.6790\n",
      "Epoch: 0001/0001 | Batch 0001/0279 | Loss: 0.6575\n",
      "Epoch: 0001/0001 | Batch 0002/0279 | Loss: 0.5981\n",
      "Epoch: 0001/0001 | Batch 0003/0279 | Loss: 0.5582\n",
      "Epoch: 0001/0001 | Batch 0004/0279 | Loss: 0.4990\n",
      "Epoch: 0001/0001 | Batch 0005/0279 | Loss: 0.5426\n",
      "Epoch: 0001/0001 | Batch 0006/0279 | Loss: 0.4273\n",
      "Epoch: 0001/0001 | Batch 0007/0279 | Loss: 0.5295\n",
      "Epoch: 0001/0001 | Batch 0008/0279 | Loss: 0.3737\n",
      "Epoch: 0001/0001 | Batch 0009/0279 | Loss: 0.3708\n",
      "Epoch: 0001/0001 | Batch 0010/0279 | Loss: 0.2252\n",
      "Epoch: 0001/0001 | Batch 0011/0279 | Loss: 0.3540\n",
      "Epoch: 0001/0001 | Batch 0012/0279 | Loss: 0.2392\n",
      "Epoch: 0001/0001 | Batch 0013/0279 | Loss: 0.2552\n",
      "Epoch: 0001/0001 | Batch 0014/0279 | Loss: 0.3226\n",
      "Epoch: 0001/0001 | Batch 0015/0279 | Loss: 0.2488\n",
      "Epoch: 0001/0001 | Batch 0016/0279 | Loss: 0.5553\n",
      "Epoch: 0001/0001 | Batch 0017/0279 | Loss: 0.1850\n",
      "Epoch: 0001/0001 | Batch 0018/0279 | Loss: 0.3547\n",
      "Epoch: 0001/0001 | Batch 0019/0279 | Loss: 0.2311\n",
      "Epoch: 0001/0001 | Batch 0020/0279 | Loss: 0.2315\n",
      "Epoch: 0001/0001 | Batch 0021/0279 | Loss: 0.2479\n",
      "Epoch: 0001/0001 | Batch 0022/0279 | Loss: 0.3879\n",
      "Epoch: 0001/0001 | Batch 0023/0279 | Loss: 0.0865\n",
      "Epoch: 0001/0001 | Batch 0024/0279 | Loss: 0.1973\n",
      "Epoch: 0001/0001 | Batch 0025/0279 | Loss: 0.1842\n",
      "Epoch: 0001/0001 | Batch 0026/0279 | Loss: 0.1172\n",
      "Epoch: 0001/0001 | Batch 0027/0279 | Loss: 0.2615\n",
      "Epoch: 0001/0001 | Batch 0028/0279 | Loss: 0.1441\n",
      "Epoch: 0001/0001 | Batch 0029/0279 | Loss: 0.2442\n",
      "Epoch: 0001/0001 | Batch 0030/0279 | Loss: 0.0986\n",
      "Epoch: 0001/0001 | Batch 0031/0279 | Loss: 0.2432\n",
      "Epoch: 0001/0001 | Batch 0032/0279 | Loss: 0.1166\n",
      "Epoch: 0001/0001 | Batch 0033/0279 | Loss: 0.1140\n",
      "Epoch: 0001/0001 | Batch 0034/0279 | Loss: 0.0368\n",
      "Epoch: 0001/0001 | Batch 0035/0279 | Loss: 0.0658\n",
      "Epoch: 0001/0001 | Batch 0036/0279 | Loss: 0.3353\n",
      "Epoch: 0001/0001 | Batch 0037/0279 | Loss: 0.0378\n",
      "Epoch: 0001/0001 | Batch 0038/0279 | Loss: 0.2106\n",
      "Epoch: 0001/0001 | Batch 0039/0279 | Loss: 0.1593\n",
      "Epoch: 0001/0001 | Batch 0040/0279 | Loss: 0.0935\n",
      "Epoch: 0001/0001 | Batch 0041/0279 | Loss: 0.1102\n",
      "Epoch: 0001/0001 | Batch 0042/0279 | Loss: 0.0469\n",
      "Epoch: 0001/0001 | Batch 0043/0279 | Loss: 0.0612\n",
      "Epoch: 0001/0001 | Batch 0044/0279 | Loss: 0.0728\n",
      "Epoch: 0001/0001 | Batch 0045/0279 | Loss: 0.0835\n",
      "Epoch: 0001/0001 | Batch 0046/0279 | Loss: 0.0201\n",
      "Epoch: 0001/0001 | Batch 0047/0279 | Loss: 0.2045\n",
      "Epoch: 0001/0001 | Batch 0048/0279 | Loss: 0.0634\n",
      "Epoch: 0001/0001 | Batch 0049/0279 | Loss: 0.0194\n",
      "Epoch: 0001/0001 | Batch 0050/0279 | Loss: 0.0562\n",
      "Epoch: 0001/0001 | Batch 0051/0279 | Loss: 0.0321\n",
      "Epoch: 0001/0001 | Batch 0052/0279 | Loss: 0.0375\n",
      "Epoch: 0001/0001 | Batch 0053/0279 | Loss: 0.1412\n",
      "Epoch: 0001/0001 | Batch 0054/0279 | Loss: 0.1439\n",
      "Epoch: 0001/0001 | Batch 0055/0279 | Loss: 0.3741\n",
      "Epoch: 0001/0001 | Batch 0056/0279 | Loss: 0.0360\n",
      "Epoch: 0001/0001 | Batch 0057/0279 | Loss: 0.1124\n",
      "Epoch: 0001/0001 | Batch 0058/0279 | Loss: 0.0467\n",
      "Epoch: 0001/0001 | Batch 0059/0279 | Loss: 0.0377\n",
      "Epoch: 0001/0001 | Batch 0060/0279 | Loss: 0.0567\n",
      "Epoch: 0001/0001 | Batch 0061/0279 | Loss: 0.0481\n",
      "Epoch: 0001/0001 | Batch 0062/0279 | Loss: 0.0399\n",
      "Epoch: 0001/0001 | Batch 0063/0279 | Loss: 0.0252\n",
      "Epoch: 0001/0001 | Batch 0064/0279 | Loss: 0.0730\n",
      "Epoch: 0001/0001 | Batch 0065/0279 | Loss: 0.2599\n",
      "Epoch: 0001/0001 | Batch 0066/0279 | Loss: 0.2110\n",
      "Epoch: 0001/0001 | Batch 0067/0279 | Loss: 0.0219\n",
      "Epoch: 0001/0001 | Batch 0068/0279 | Loss: 0.0315\n",
      "Epoch: 0001/0001 | Batch 0069/0279 | Loss: 0.0284\n",
      "Epoch: 0001/0001 | Batch 0070/0279 | Loss: 0.0230\n",
      "Epoch: 0001/0001 | Batch 0071/0279 | Loss: 0.0190\n",
      "Epoch: 0001/0001 | Batch 0072/0279 | Loss: 0.0832\n",
      "Epoch: 0001/0001 | Batch 0073/0279 | Loss: 0.0135\n",
      "Epoch: 0001/0001 | Batch 0074/0279 | Loss: 0.0510\n",
      "Epoch: 0001/0001 | Batch 0075/0279 | Loss: 0.0141\n",
      "Epoch: 0001/0001 | Batch 0076/0279 | Loss: 0.0176\n",
      "Epoch: 0001/0001 | Batch 0077/0279 | Loss: 0.0340\n",
      "Epoch: 0001/0001 | Batch 0078/0279 | Loss: 0.0219\n",
      "Epoch: 0001/0001 | Batch 0079/0279 | Loss: 0.0145\n",
      "Epoch: 0001/0001 | Batch 0080/0279 | Loss: 0.0199\n",
      "Epoch: 0001/0001 | Batch 0081/0279 | Loss: 0.1538\n",
      "Epoch: 0001/0001 | Batch 0082/0279 | Loss: 0.0163\n",
      "Epoch: 0001/0001 | Batch 0083/0279 | Loss: 0.1345\n",
      "Epoch: 0001/0001 | Batch 0084/0279 | Loss: 0.1474\n",
      "Epoch: 0001/0001 | Batch 0085/0279 | Loss: 0.0449\n",
      "Epoch: 0001/0001 | Batch 0086/0279 | Loss: 0.0397\n",
      "Epoch: 0001/0001 | Batch 0087/0279 | Loss: 0.0958\n",
      "Epoch: 0001/0001 | Batch 0088/0279 | Loss: 0.0159\n",
      "Epoch: 0001/0001 | Batch 0089/0279 | Loss: 0.0286\n",
      "Epoch: 0001/0001 | Batch 0090/0279 | Loss: 0.0373\n",
      "Epoch: 0001/0001 | Batch 0091/0279 | Loss: 0.0110\n",
      "Epoch: 0001/0001 | Batch 0092/0279 | Loss: 0.1690\n",
      "Epoch: 0001/0001 | Batch 0093/0279 | Loss: 0.0114\n",
      "Epoch: 0001/0001 | Batch 0094/0279 | Loss: 0.0264\n",
      "Epoch: 0001/0001 | Batch 0095/0279 | Loss: 0.0089\n",
      "Epoch: 0001/0001 | Batch 0096/0279 | Loss: 0.0098\n",
      "Epoch: 0001/0001 | Batch 0097/0279 | Loss: 0.0126\n",
      "Epoch: 0001/0001 | Batch 0098/0279 | Loss: 0.0193\n",
      "Epoch: 0001/0001 | Batch 0099/0279 | Loss: 0.0083\n",
      "Epoch: 0001/0001 | Batch 0100/0279 | Loss: 0.0235\n",
      "Epoch: 0001/0001 | Batch 0101/0279 | Loss: 0.3367\n",
      "Epoch: 0001/0001 | Batch 0102/0279 | Loss: 0.0437\n",
      "Epoch: 0001/0001 | Batch 0103/0279 | Loss: 0.0139\n",
      "Epoch: 0001/0001 | Batch 0104/0279 | Loss: 0.0539\n",
      "Epoch: 0001/0001 | Batch 0105/0279 | Loss: 0.0143\n",
      "Epoch: 0001/0001 | Batch 0106/0279 | Loss: 0.0082\n",
      "Epoch: 0001/0001 | Batch 0107/0279 | Loss: 0.0061\n",
      "Epoch: 0001/0001 | Batch 0108/0279 | Loss: 0.0090\n",
      "Epoch: 0001/0001 | Batch 0109/0279 | Loss: 0.0233\n",
      "Epoch: 0001/0001 | Batch 0110/0279 | Loss: 0.0076\n",
      "Epoch: 0001/0001 | Batch 0111/0279 | Loss: 0.3133\n",
      "Epoch: 0001/0001 | Batch 0112/0279 | Loss: 0.0074\n",
      "Epoch: 0001/0001 | Batch 0113/0279 | Loss: 0.0121\n",
      "Epoch: 0001/0001 | Batch 0114/0279 | Loss: 0.0682\n",
      "Epoch: 0001/0001 | Batch 0115/0279 | Loss: 0.0097\n",
      "Epoch: 0001/0001 | Batch 0116/0279 | Loss: 0.0077\n",
      "Epoch: 0001/0001 | Batch 0117/0279 | Loss: 0.0110\n",
      "Epoch: 0001/0001 | Batch 0118/0279 | Loss: 0.0075\n",
      "Epoch: 0001/0001 | Batch 0119/0279 | Loss: 0.0076\n",
      "Epoch: 0001/0001 | Batch 0120/0279 | Loss: 0.1662\n",
      "Epoch: 0001/0001 | Batch 0121/0279 | Loss: 0.2932\n",
      "Epoch: 0001/0001 | Batch 0122/0279 | Loss: 0.0097\n",
      "Epoch: 0001/0001 | Batch 0123/0279 | Loss: 0.5066\n",
      "Epoch: 0001/0001 | Batch 0124/0279 | Loss: 0.1139\n",
      "Epoch: 0001/0001 | Batch 0125/0279 | Loss: 0.0192\n",
      "Epoch: 0001/0001 | Batch 0126/0279 | Loss: 0.0140\n",
      "Epoch: 0001/0001 | Batch 0127/0279 | Loss: 0.0390\n",
      "Epoch: 0001/0001 | Batch 0128/0279 | Loss: 0.1408\n",
      "Epoch: 0001/0001 | Batch 0129/0279 | Loss: 0.0818\n",
      "Epoch: 0001/0001 | Batch 0130/0279 | Loss: 0.0508\n",
      "Epoch: 0001/0001 | Batch 0131/0279 | Loss: 0.2146\n",
      "Epoch: 0001/0001 | Batch 0132/0279 | Loss: 0.0538\n",
      "Epoch: 0001/0001 | Batch 0133/0279 | Loss: 0.0724\n",
      "Epoch: 0001/0001 | Batch 0134/0279 | Loss: 0.0104\n",
      "Epoch: 0001/0001 | Batch 0135/0279 | Loss: 0.0095\n",
      "Epoch: 0001/0001 | Batch 0136/0279 | Loss: 0.0111\n",
      "Epoch: 0001/0001 | Batch 0137/0279 | Loss: 0.0117\n",
      "Epoch: 0001/0001 | Batch 0138/0279 | Loss: 0.0609\n",
      "Epoch: 0001/0001 | Batch 0139/0279 | Loss: 0.0101\n",
      "Epoch: 0001/0001 | Batch 0140/0279 | Loss: 0.5502\n",
      "Epoch: 0001/0001 | Batch 0141/0279 | Loss: 0.2854\n",
      "Epoch: 0001/0001 | Batch 0142/0279 | Loss: 0.0199\n",
      "Epoch: 0001/0001 | Batch 0143/0279 | Loss: 0.0263\n",
      "Epoch: 0001/0001 | Batch 0144/0279 | Loss: 0.0144\n",
      "Epoch: 0001/0001 | Batch 0145/0279 | Loss: 0.0171\n",
      "Epoch: 0001/0001 | Batch 0146/0279 | Loss: 0.5886\n",
      "Epoch: 0001/0001 | Batch 0147/0279 | Loss: 0.0147\n",
      "Epoch: 0001/0001 | Batch 0148/0279 | Loss: 0.2811\n",
      "Epoch: 0001/0001 | Batch 0149/0279 | Loss: 0.0179\n",
      "Epoch: 0001/0001 | Batch 0150/0279 | Loss: 0.0384\n",
      "Epoch: 0001/0001 | Batch 0151/0279 | Loss: 0.1078\n",
      "Epoch: 0001/0001 | Batch 0152/0279 | Loss: 0.1791\n",
      "Epoch: 0001/0001 | Batch 0153/0279 | Loss: 0.0717\n",
      "Epoch: 0001/0001 | Batch 0154/0279 | Loss: 0.0276\n",
      "Epoch: 0001/0001 | Batch 0155/0279 | Loss: 0.0307\n",
      "Epoch: 0001/0001 | Batch 0156/0279 | Loss: 0.0281\n",
      "Epoch: 0001/0001 | Batch 0157/0279 | Loss: 0.0337\n",
      "Epoch: 0001/0001 | Batch 0158/0279 | Loss: 0.0311\n",
      "Epoch: 0001/0001 | Batch 0159/0279 | Loss: 0.0215\n",
      "Epoch: 0001/0001 | Batch 0160/0279 | Loss: 0.0890\n",
      "Epoch: 0001/0001 | Batch 0161/0279 | Loss: 0.2693\n",
      "Epoch: 0001/0001 | Batch 0162/0279 | Loss: 0.0169\n",
      "Epoch: 0001/0001 | Batch 0163/0279 | Loss: 0.0229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001/0001 | Batch 0164/0279 | Loss: 0.0299\n",
      "Epoch: 0001/0001 | Batch 0165/0279 | Loss: 0.0272\n",
      "Epoch: 0001/0001 | Batch 0166/0279 | Loss: 0.0142\n",
      "Epoch: 0001/0001 | Batch 0167/0279 | Loss: 0.0479\n",
      "Epoch: 0001/0001 | Batch 0168/0279 | Loss: 0.0151\n",
      "Epoch: 0001/0001 | Batch 0169/0279 | Loss: 0.0187\n",
      "Epoch: 0001/0001 | Batch 0170/0279 | Loss: 0.0740\n",
      "Epoch: 0001/0001 | Batch 0171/0279 | Loss: 0.0161\n",
      "Epoch: 0001/0001 | Batch 0172/0279 | Loss: 0.0103\n",
      "Epoch: 0001/0001 | Batch 0173/0279 | Loss: 0.0097\n",
      "Epoch: 0001/0001 | Batch 0174/0279 | Loss: 0.0110\n",
      "Epoch: 0001/0001 | Batch 0175/0279 | Loss: 0.0094\n",
      "Epoch: 0001/0001 | Batch 0176/0279 | Loss: 0.1847\n",
      "Epoch: 0001/0001 | Batch 0177/0279 | Loss: 0.0130\n",
      "Epoch: 0001/0001 | Batch 0178/0279 | Loss: 0.1498\n",
      "Epoch: 0001/0001 | Batch 0179/0279 | Loss: 0.0102\n",
      "Epoch: 0001/0001 | Batch 0180/0279 | Loss: 0.0088\n",
      "Epoch: 0001/0001 | Batch 0181/0279 | Loss: 0.2285\n",
      "Epoch: 0001/0001 | Batch 0182/0279 | Loss: 0.0209\n",
      "Epoch: 0001/0001 | Batch 0183/0279 | Loss: 0.0101\n",
      "Epoch: 0001/0001 | Batch 0184/0279 | Loss: 0.0090\n",
      "Epoch: 0001/0001 | Batch 0185/0279 | Loss: 0.0057\n",
      "Epoch: 0001/0001 | Batch 0186/0279 | Loss: 0.0126\n",
      "Epoch: 0001/0001 | Batch 0187/0279 | Loss: 0.0055\n",
      "Epoch: 0001/0001 | Batch 0188/0279 | Loss: 0.2072\n",
      "Epoch: 0001/0001 | Batch 0189/0279 | Loss: 0.0087\n",
      "Epoch: 0001/0001 | Batch 0190/0279 | Loss: 0.0082\n",
      "Epoch: 0001/0001 | Batch 0191/0279 | Loss: 0.4800\n",
      "Epoch: 0001/0001 | Batch 0192/0279 | Loss: 0.0099\n",
      "Epoch: 0001/0001 | Batch 0193/0279 | Loss: 0.0100\n",
      "Epoch: 0001/0001 | Batch 0194/0279 | Loss: 0.0244\n",
      "Epoch: 0001/0001 | Batch 0195/0279 | Loss: 0.0141\n",
      "Epoch: 0001/0001 | Batch 0196/0279 | Loss: 0.3099\n",
      "Epoch: 0001/0001 | Batch 0197/0279 | Loss: 0.0202\n",
      "Epoch: 0001/0001 | Batch 0198/0279 | Loss: 0.0425\n",
      "Epoch: 0001/0001 | Batch 0199/0279 | Loss: 0.0093\n",
      "Epoch: 0001/0001 | Batch 0200/0279 | Loss: 0.0307\n",
      "Epoch: 0001/0001 | Batch 0201/0279 | Loss: 0.1113\n",
      "Epoch: 0001/0001 | Batch 0202/0279 | Loss: 0.1795\n",
      "Epoch: 0001/0001 | Batch 0203/0279 | Loss: 0.0276\n",
      "Epoch: 0001/0001 | Batch 0204/0279 | Loss: 0.0409\n",
      "Epoch: 0001/0001 | Batch 0205/0279 | Loss: 0.0225\n",
      "Epoch: 0001/0001 | Batch 0206/0279 | Loss: 0.0124\n",
      "Epoch: 0001/0001 | Batch 0207/0279 | Loss: 0.0127\n",
      "Epoch: 0001/0001 | Batch 0208/0279 | Loss: 0.0146\n",
      "Epoch: 0001/0001 | Batch 0209/0279 | Loss: 0.0221\n",
      "Epoch: 0001/0001 | Batch 0210/0279 | Loss: 0.0956\n",
      "Epoch: 0001/0001 | Batch 0211/0279 | Loss: 0.0163\n",
      "Epoch: 0001/0001 | Batch 0212/0279 | Loss: 0.0292\n",
      "Epoch: 0001/0001 | Batch 0213/0279 | Loss: 0.0575\n",
      "Epoch: 0001/0001 | Batch 0214/0279 | Loss: 0.0073\n",
      "Epoch: 0001/0001 | Batch 0215/0279 | Loss: 0.0114\n",
      "Epoch: 0001/0001 | Batch 0216/0279 | Loss: 0.0239\n",
      "Epoch: 0001/0001 | Batch 0217/0279 | Loss: 0.0071\n",
      "Epoch: 0001/0001 | Batch 0218/0279 | Loss: 0.0112\n",
      "Epoch: 0001/0001 | Batch 0219/0279 | Loss: 0.0090\n",
      "Epoch: 0001/0001 | Batch 0220/0279 | Loss: 0.0085\n",
      "Epoch: 0001/0001 | Batch 0221/0279 | Loss: 0.0067\n",
      "Epoch: 0001/0001 | Batch 0222/0279 | Loss: 0.0091\n",
      "Epoch: 0001/0001 | Batch 0223/0279 | Loss: 0.0270\n",
      "Epoch: 0001/0001 | Batch 0224/0279 | Loss: 0.0062\n",
      "Epoch: 0001/0001 | Batch 0225/0279 | Loss: 0.0808\n",
      "Epoch: 0001/0001 | Batch 0226/0279 | Loss: 0.0065\n",
      "Epoch: 0001/0001 | Batch 0227/0279 | Loss: 0.0182\n",
      "Epoch: 0001/0001 | Batch 0228/0279 | Loss: 0.1458\n",
      "Epoch: 0001/0001 | Batch 0229/0279 | Loss: 0.0276\n",
      "Epoch: 0001/0001 | Batch 0230/0279 | Loss: 0.0079\n",
      "Epoch: 0001/0001 | Batch 0231/0279 | Loss: 0.0056\n",
      "Epoch: 0001/0001 | Batch 0232/0279 | Loss: 0.0062\n",
      "Epoch: 0001/0001 | Batch 0233/0279 | Loss: 0.0051\n",
      "Epoch: 0001/0001 | Batch 0234/0279 | Loss: 0.0053\n",
      "Epoch: 0001/0001 | Batch 0235/0279 | Loss: 0.0045\n",
      "Epoch: 0001/0001 | Batch 0236/0279 | Loss: 0.0056\n",
      "Epoch: 0001/0001 | Batch 0237/0279 | Loss: 0.0052\n",
      "Epoch: 0001/0001 | Batch 0238/0279 | Loss: 0.3837\n",
      "Epoch: 0001/0001 | Batch 0239/0279 | Loss: 0.0062\n",
      "Epoch: 0001/0001 | Batch 0240/0279 | Loss: 0.0205\n",
      "Epoch: 0001/0001 | Batch 0241/0279 | Loss: 0.0103\n",
      "Epoch: 0001/0001 | Batch 0242/0279 | Loss: 0.0083\n",
      "Epoch: 0001/0001 | Batch 0243/0279 | Loss: 0.0074\n",
      "Epoch: 0001/0001 | Batch 0244/0279 | Loss: 0.0082\n",
      "Epoch: 0001/0001 | Batch 0245/0279 | Loss: 0.0062\n",
      "Epoch: 0001/0001 | Batch 0246/0279 | Loss: 0.0066\n",
      "Epoch: 0001/0001 | Batch 0247/0279 | Loss: 0.0044\n",
      "Epoch: 0001/0001 | Batch 0248/0279 | Loss: 0.0055\n",
      "Epoch: 0001/0001 | Batch 0249/0279 | Loss: 0.0061\n",
      "Epoch: 0001/0001 | Batch 0250/0279 | Loss: 0.0057\n",
      "Epoch: 0001/0001 | Batch 0251/0279 | Loss: 0.0063\n",
      "Epoch: 0001/0001 | Batch 0252/0279 | Loss: 0.3868\n",
      "Epoch: 0001/0001 | Batch 0253/0279 | Loss: 0.0049\n",
      "Epoch: 0001/0001 | Batch 0254/0279 | Loss: 0.0451\n",
      "Epoch: 0001/0001 | Batch 0255/0279 | Loss: 0.0054\n",
      "Epoch: 0001/0001 | Batch 0256/0279 | Loss: 0.2178\n",
      "Epoch: 0001/0001 | Batch 0257/0279 | Loss: 0.0041\n",
      "Epoch: 0001/0001 | Batch 0258/0279 | Loss: 0.0091\n",
      "Epoch: 0001/0001 | Batch 0259/0279 | Loss: 0.0068\n",
      "Epoch: 0001/0001 | Batch 0260/0279 | Loss: 0.0184\n",
      "Epoch: 0001/0001 | Batch 0261/0279 | Loss: 0.0047\n",
      "Epoch: 0001/0001 | Batch 0262/0279 | Loss: 0.0052\n",
      "Epoch: 0001/0001 | Batch 0263/0279 | Loss: 0.0050\n",
      "Epoch: 0001/0001 | Batch 0264/0279 | Loss: 0.0065\n",
      "Epoch: 0001/0001 | Batch 0265/0279 | Loss: 0.0376\n",
      "Epoch: 0001/0001 | Batch 0266/0279 | Loss: 0.0290\n",
      "Epoch: 0001/0001 | Batch 0267/0279 | Loss: 0.0061\n",
      "Epoch: 0001/0001 | Batch 0268/0279 | Loss: 0.1966\n",
      "Epoch: 0001/0001 | Batch 0269/0279 | Loss: 0.0052\n",
      "Epoch: 0001/0001 | Batch 0270/0279 | Loss: 0.0197\n",
      "Epoch: 0001/0001 | Batch 0271/0279 | Loss: 0.0064\n",
      "Epoch: 0001/0001 | Batch 0272/0279 | Loss: 0.1323\n",
      "Epoch: 0001/0001 | Batch 0273/0279 | Loss: 0.0050\n",
      "Epoch: 0001/0001 | Batch 0274/0279 | Loss: 0.0208\n",
      "Epoch: 0001/0001 | Batch 0275/0279 | Loss: 0.0125\n",
      "Epoch: 0001/0001 | Batch 0276/0279 | Loss: 0.0154\n",
      "Epoch: 0001/0001 | Batch 0277/0279 | Loss: 0.0067\n",
      "Epoch: 0001/0001 | Batch 0278/0279 | Loss: 0.0184\n",
      "training accuracy: 99.26%\n",
      "Time elapsed: 3.72 min\n",
      "Total Training Time: 3.72 min\n",
      "Test accuracy: 99.01%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range (epochs):\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_masks = batch['attention_mask'].to(DEVICE)\n",
    "        label = batch['label'].to(DEVICE)\n",
    "        \n",
    "        outputs = model(input_ids, attention_masks, labels = label)\n",
    "        loss, logits = outputs['loss'], outputs['logits']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print (f'Epoch: {epoch+1:04d}/{epochs:04d} | '\n",
    "                   f'Batch {batch_idx:04d}/{len(train_loader):04d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "            \n",
    "    model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{accuracy(model, train_loader, DEVICE):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {accuracy(model, test_loader, DEVICE):.2f}%')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64bd3cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"trained1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e2b25190",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"trained1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d313a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text =\"INTERAC e-Transfer: JACQUELINE ANDRADE SANCHEZ sent you money. See https://et.interac.ca/sl/wzVjShR98WIy. Data rates may apply\"\n",
    "tokenized_message = tokenizer(input_text, truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1114fed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**tokenized_message)\n",
    "    predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c320b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
